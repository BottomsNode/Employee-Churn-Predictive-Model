{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Employee Churn Model: a HR Analytics Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Problem-Definition\" data-toc-modified-id=\"Problem-Definition-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Problem Definition</a></span><ul class=\"toc-item\"><li><span><a href=\"#Project-Overview\" data-toc-modified-id=\"Project-Overview-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Project Overview</a></span></li><li><span><a href=\"#Problem-Statement\" data-toc-modified-id=\"Problem-Statement-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Problem Statement</a></span></li></ul></li><li><span><a href=\"#Dataset-Analysis\" data-toc-modified-id=\"Dataset-Analysis-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Dataset Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importing-Python-libraries\" data-toc-modified-id=\"Importing-Python-libraries-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Importing Python libraries</a></span></li><li><span><a href=\"#Importing-the-data\" data-toc-modified-id=\"Importing-the-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Importing the data</a></span></li><li><span><a href=\"#Data-Description-and-Exploratory-Visualisations\" data-toc-modified-id=\"Data-Description-and-Exploratory-Visualisations-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Data Description and Exploratory Visualisations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Overview</a></span></li><li><span><a href=\"#Numerical-features-overview\" data-toc-modified-id=\"Numerical-features-overview-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Numerical features overview</a></span></li></ul></li><li><span><a href=\"#Feature-distribution-by-target-attribute\" data-toc-modified-id=\"Feature-distribution-by-target-attribute-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Feature distribution by target attribute</a></span><ul class=\"toc-item\"><li><span><a href=\"#Age\" data-toc-modified-id=\"Age-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Age</a></span></li><li><span><a href=\"#Education\" data-toc-modified-id=\"Education-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Education</a></span></li><li><span><a href=\"#Gender\" data-toc-modified-id=\"Gender-2.4.3\"><span class=\"toc-item-num\">2.4.3&nbsp;&nbsp;</span>Gender</a></span></li><li><span><a href=\"#Marital-Status\" data-toc-modified-id=\"Marital-Status-2.4.4\"><span class=\"toc-item-num\">2.4.4&nbsp;&nbsp;</span>Marital Status</a></span></li><li><span><a href=\"#Distance-from-Home\" data-toc-modified-id=\"Distance-from-Home-2.4.5\"><span class=\"toc-item-num\">2.4.5&nbsp;&nbsp;</span>Distance from Home</a></span></li><li><span><a href=\"#Department\" data-toc-modified-id=\"Department-2.4.6\"><span class=\"toc-item-num\">2.4.6&nbsp;&nbsp;</span>Department</a></span></li><li><span><a href=\"#Role-and-Work-Conditions\" data-toc-modified-id=\"Role-and-Work-Conditions-2.4.7\"><span class=\"toc-item-num\">2.4.7&nbsp;&nbsp;</span>Role and Work Conditions</a></span></li><li><span><a href=\"#Years-at-the-Company\" data-toc-modified-id=\"Years-at-the-Company-2.4.8\"><span class=\"toc-item-num\">2.4.8&nbsp;&nbsp;</span>Years at the Company</a></span></li><li><span><a href=\"#Years-With-Current-Manager\" data-toc-modified-id=\"Years-With-Current-Manager-2.4.9\"><span class=\"toc-item-num\">2.4.9&nbsp;&nbsp;</span>Years With Current Manager</a></span></li><li><span><a href=\"#Work-Life-Balance-Score\" data-toc-modified-id=\"Work-Life-Balance-Score-2.4.10\"><span class=\"toc-item-num\">2.4.10&nbsp;&nbsp;</span>Work-Life Balance Score</a></span></li><li><span><a href=\"#Pay/Salary-Employee-Information\" data-toc-modified-id=\"Pay/Salary-Employee-Information-2.4.11\"><span class=\"toc-item-num\">2.4.11&nbsp;&nbsp;</span>Pay/Salary Employee Information</a></span></li><li><span><a href=\"#Employee-Satisfaction-and-Performance-Information\" data-toc-modified-id=\"Employee-Satisfaction-and-Performance-Information-2.4.12\"><span class=\"toc-item-num\">2.4.12&nbsp;&nbsp;</span>Employee Satisfaction and Performance Information</a></span></li></ul></li><li><span><a href=\"#Target-Variable:-Attrition\" data-toc-modified-id=\"Target-Variable:-Attrition-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Target Variable: Attrition</a></span></li><li><span><a href=\"#Correlation\" data-toc-modified-id=\"Correlation-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Correlation</a></span></li><li><span><a href=\"#EDA-Concluding-Remarks\" data-toc-modified-id=\"EDA-Concluding-Remarks-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>EDA Concluding Remarks</a></span></li></ul></li><li><span><a href=\"#Pre-processing-Pipeline\" data-toc-modified-id=\"Pre-processing-Pipeline-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Pre-processing Pipeline</a></span><ul class=\"toc-item\"><li><span><a href=\"#Encoding\" data-toc-modified-id=\"Encoding-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Encoding</a></span></li><li><span><a href=\"#Feature-Scaling\" data-toc-modified-id=\"Feature-Scaling-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Feature Scaling</a></span></li><li><span><a href=\"#Splitting-data-into-training-and-testing-sets\" data-toc-modified-id=\"Splitting-data-into-training-and-testing-sets-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Splitting data into training and testing sets</a></span></li></ul></li><li><span><a href=\"#Building-Machine-Learning-Models\" data-toc-modified-id=\"Building-Machine-Learning-Models-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Building Machine Learning Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Baseline-Algorithms\" data-toc-modified-id=\"Baseline-Algorithms-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Baseline Algorithms</a></span></li><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Fine-tuning\" data-toc-modified-id=\"Fine-tuning-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Fine-tuning</a></span></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Evaluation</a></span></li></ul></li><li><span><a href=\"#Random-Forest-Classifier\" data-toc-modified-id=\"Random-Forest-Classifier-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Random Forest Classifier</a></span><ul class=\"toc-item\"><li><span><a href=\"#Fine-tuning\" data-toc-modified-id=\"Fine-tuning-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Fine-tuning</a></span></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Evaluation</a></span></li></ul></li><li><span><a href=\"#ROC-Graphs\" data-toc-modified-id=\"ROC-Graphs-4.4\"><span class=\"toc-item-num\">&nbsp;&nbsp;</span></a></span></li></ul></li><li><span><a href=\"#Concluding-Remarks\" data-toc-modified-id=\"Concluding-Remarks-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Concluding Remarks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Risk-Category\" data-toc-modified-id=\"Risk-Category-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Risk Category</a></span></li><li><span><a href=\"#Strategic-Retention-Plan\" data-toc-modified-id=\"Strategic-Retention-Plan-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Strategic Retention Plan</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Employee turn-over (also known as \"employee churn\") is a costly problem for companies. The true cost of replacing an employee\n",
    "can often be quite large. A study by the [Center for American Progress](https://www.americanprogress.org/wp-content/uploads/2012/11/CostofTurnover.pdf) found that companies typically pay about one-fifth of an employeeâ€™s salary to replace that employee, and the cost can significantly increase if executives or highest-paid employees are to be replaced. <br>\n",
    "\n",
    "In other words, the cost of replacing employees for most employers remains significant. This is due to the amount of time spent to interview and find a replacement, sign-on bonuses, and the loss of productivity for several months while the new employee gets accustomed to the new role. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding why and when employees are most likely to leave can lead to actions to improve employee retention as well as possibly planning new hiring in advance. I will be usign a step-by-step systematic approach using a method that could be used for a variety of ML problems. This project would fall under what is commonly known as \"**HR Anlytics**\", \"**People Analytics**\". <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this study, we will attempt to solve the following problem statement is: <br>\n",
    "> ** What is the likelihood of an active employee leaving the company? <br>\n",
    "What are the key indicators of an employee leaving the company? <br>\n",
    "What policies or strategies can be adopted based on the results to improve employee retention? **\n",
    "\n",
    "Given that we have data on former employees, this is a standard **supervised classification problem** where the label is a binary variable, 0 (active employee), 1 (former employee). In this study, our target variable Y is the probability of an employee leaving the company. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://images.unsplash.com/photo-1523006520266-d3a4a8152803?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1422&q=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case study, a HR dataset was sourced from [IBM HR Analytics Employee Attrition & Performance](https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/) which contains employee data for 1,470 employees with various information about the employees. I will use this dataset to predict when employees are going to quit by understanding the main drivers of employee churn. <br>\n",
    "\n",
    "> As stated on the [IBM website](https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/) *\"This is a fictional data set created by IBM data scientists\"*. <br>\n",
    "Its main purpose was to demonstrate the IBM Watson Analytics tool for employee attrition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:03:57.588868Z",
     "start_time": "2019-02-26T17:03:55.460102Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing libraries for data handling and analysis\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "from openpyxl import load_workbook\n",
    "import numpy as np\n",
    "from scipy.stats import norm, skew\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:03:58.509004Z",
     "start_time": "2019-02-26T17:03:57.735796Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing libraries for data visualisations\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "color = sns.color_palette()\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# Standard plotly imports\n",
    "import plotly\n",
    "import chart_studio.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "# Using plotly + cufflinks in offline mode\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf\n",
    "cf.set_config_file(offline=True)\n",
    "import cufflinks\n",
    "cufflinks.go_offline(connected=True)\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:04:04.775128Z",
     "start_time": "2019-02-26T17:04:03.078922Z"
    }
   },
   "outputs": [],
   "source": [
    "# sklearn modules for preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE  # SMOTE\n",
    "\n",
    "# sklearn modules for ML model selection\n",
    "from sklearn.model_selection import train_test_split  # import 'train_test_split'\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Libraries for data modelling\n",
    "from sklearn import svm, tree, linear_model, neighbors\n",
    "from sklearn import naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Common sklearn Model Helpers\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "# from sklearn.datasets import make_classification\n",
    "\n",
    "# sklearn modules for performance metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve\n",
    "from sklearn.metrics import auc, roc_auc_score, roc_curve, recall_score, log_loss\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:03:58.615595Z",
     "start_time": "2019-02-26T17:03:58.611072Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing misceallenous libraries\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import timeit\n",
    "import string\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "from dateutil.parser import parse\n",
    "import jupyternotify\n",
    "ip = get_ipython()\n",
    "ip.register_magics(jupyternotify.JupyterNotifyMagics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's import the dataset and make of a copy of the source file for this analysis. <br> The dataset contains 1,470 rows and 35 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:04:13.722057Z",
     "start_time": "2019-02-26T17:04:13.190727Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read Excel file\n",
    "df_sourcefile = pd.read_excel(\n",
    "    'Data/WA_Fn-UseC_-HR-Employee-Attrition.xlsx', sheet_name=0)\n",
    "print(\"Shape of dataframe is: {}\".format(df_sourcefile.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:04:14.574804Z",
     "start_time": "2019-02-26T17:04:14.566410Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make a copy of the original sourcefile\n",
    "df_HR = df_sourcefile.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description and Exploratory Visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this section, we will provide data visualizations that summarizes or extracts relevant characteristics of features in our dataset. Let's look at each column in detail, get a better understanding of the dataset, and group them together when appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:04:15.992407Z",
     "start_time": "2019-02-26T17:04:15.984336Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset columns\n",
    "df_HR.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:04:17.560239Z",
     "start_time": "2019-02-26T17:04:17.496713Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dataset header\n",
    "df_HR.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The dataset contains several numerical and categorical columns providing various information on employee's personal and employment details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:04:28.699004Z",
     "start_time": "2019-02-26T17:04:28.678744Z"
    }
   },
   "outputs": [],
   "source": [
    "df_HR.columns.to_series().groupby(df_HR.dtypes).groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:04:31.771322Z",
     "start_time": "2019-02-26T17:04:31.760624Z"
    }
   },
   "outputs": [],
   "source": [
    "# Columns datatypes and missign values\n",
    "df_HR.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The data provided has no missing values. In HR Analytics, employee data is unlikely to feature large ratio of missing values as HR Departments typically have all personal and employment data on-file. However, the type of documentation data is being kept in (i.e. whether it is paper-based, Excel spreadhsheets, databases, etc) has a massive impact on the accuracy and the ease of access to the HR data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical features overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:07:46.256407Z",
     "start_time": "2019-02-26T17:07:46.157177Z"
    }
   },
   "outputs": [],
   "source": [
    "df_HR.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:10:32.790863Z",
     "start_time": "2019-02-26T17:10:30.177055Z"
    }
   },
   "outputs": [],
   "source": [
    "df_HR.hist(figsize=(20,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A few observations can be made based on the information and histograms for numerical features:\n",
    " - Many histograms are tail-heavy; indeed several distributions are right-skewed (e.g. MonthlyIncome DistanceFromHome, YearsAtCompany). Data transformation methods may be required to approach a normal distribution prior to fitting a model to the data.\n",
    " - Age distribution is a slightly right-skewed normal distribution with the bulk of the staff between 25 and 45 years old.\n",
    " - EmployeeCount and StandardHours are constant values for all employees. They're likely to be redundant features.\n",
    " - Employee Number is likely to be a unique identifier for employees given the feature's quasi-uniform distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distribution by target attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The age distributions for Active and Ex-employees only differs by one year. <br>\n",
    "The average age of ex-employees is **33.6** years old, while **37.6** is the average age for current employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:17:11.519877Z",
     "start_time": "2019-02-26T17:17:11.505111Z"
    }
   },
   "outputs": [],
   "source": [
    "(mu, sigma) = norm.fit(df_HR.loc[df_HR['Attrition'] == 'Yes', 'Age'])\n",
    "print(\n",
    "    'Ex-exmployees: average age = {:.1f} years old and standard deviation = {:.1f}'.format(mu, sigma))\n",
    "(mu, sigma) = norm.fit(df_HR.loc[df_HR['Attrition'] == 'No', 'Age'])\n",
    "print('Current exmployees: average age = {:.1f} years old and standard deviation = {:.1f}'.format(\n",
    "    mu, sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's create a kernel density estimation (KDE) plot colored by the value of the target. A kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:22:59.766353Z",
     "start_time": "2019-02-26T17:22:57.740340Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Extract data for histograms\n",
    "x1 = df_HR.loc[df_HR['Attrition'] == 'No', 'Age']\n",
    "x2 = df_HR.loc[df_HR['Attrition'] == 'Yes', 'Age']\n",
    "\n",
    "# Group data together\n",
    "hist_data = [x1, x2]\n",
    "group_labels = ['Active Employees', 'Ex-Employees']\n",
    "\n",
    "# Create histogram plot\n",
    "fig = go.Figure()\n",
    "\n",
    "for i, data in enumerate(hist_data):\n",
    "    fig.add_trace(go.Histogram(x=data, name=group_labels[i], nbinsx=12))\n",
    "\n",
    "# Add title\n",
    "fig.update_layout(title='Age Distribution in Percent by Attrition Status')\n",
    "fig.update_xaxes(range=[15, 60], dtick=5)\n",
    "\n",
    "# Plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Education"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Several Education Fields are represented in the dataset, namely: Human Resources, Life Sciences, Marketing, Medical, Technical Degree, and a miscellaneous category Other. Here, I plot the normalized % of Leavers for each Education Field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Education Field of employees\n",
    "df_HR['EducationField'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_EducationField = pd.DataFrame(columns=[\"Field\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['EducationField'].unique()):\n",
    "    ratio = df_HR[(df_HR['EducationField']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['EducationField']==field].shape[0]\n",
    "    df_EducationField.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_EF = df_EducationField.groupby(by=\"Field\").sum()\n",
    "df_EF.iplot(kind='bar',title='Leavers by Education Field (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Gender distribution shows that the dataset features a higher relative proportion of male ex-employees than female ex-employees, with normalised gender distribution of ex-employees in the dataset at 17.0% for Males and 14.8% for Females."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender of employees\n",
    "df_HR['Gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:24:03.445736Z",
     "start_time": "2019-02-26T17:24:03.430550Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Normalised gender distribution of ex-employees in the dataset: Male = {:.1f}%; Female {:.1f}%.\".format((df_HR[(df_HR['Attrition'] == 'Yes') & (\n",
    "    df_HR['Gender'] == 'Male')].shape[0] / df_HR[df_HR['Gender'] == 'Male'].shape[0])*100, (df_HR[(df_HR['Attrition'] == 'Yes') & (df_HR['Gender'] == 'Female')].shape[0] / df_HR[df_HR['Gender'] == 'Female'].shape[0])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Gender = pd.DataFrame(columns=[\"Gender\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['Gender'].unique()):\n",
    "    ratio = df_HR[(df_HR['Gender']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['Gender']==field].shape[0]\n",
    "    df_Gender.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_G = df_Gender.groupby(by=\"Gender\").sum()\n",
    "df_G.iplot(kind='bar',title='Leavers by Gender (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Marital Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The dataset features three marital status: Married (673 employees), Single (470 employees), Divorced (327 employees). <br>\n",
    "Single employees show the largest proportion of leavers at 25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:24:23.536407Z",
     "start_time": "2019-02-26T17:24:23.527184Z"
    }
   },
   "outputs": [],
   "source": [
    "# Marital Status of employees\n",
    "df_HR['MaritalStatus'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Marital = pd.DataFrame(columns=[\"Marital Status\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['MaritalStatus'].unique()):\n",
    "    ratio = df_HR[(df_HR['MaritalStatus']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['MaritalStatus']==field].shape[0]\n",
    "    df_Marital.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_MF = df_Marital.groupby(by=\"Marital Status\").sum()\n",
    "df_MF.iplot(kind='bar',title='Leavers by Marital Status (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance from Home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Distance from home for employees to get to work varies from 1 to 29 miles. There is no discernable strong correlation between Distance from Home and Attrition Status as per the KDE plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:24:39.197221Z",
     "start_time": "2019-02-26T17:24:39.179336Z"
    }
   },
   "outputs": [],
   "source": [
    "# Distance from Home\n",
    "print(\"Distance from home for employees to get to work is from {} to {} miles.\".format(df_HR['DistanceFromHome'].min(),\n",
    "                                                                                       df_HR['DistanceFromHome'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:24:39.868875Z",
     "start_time": "2019-02-26T17:24:39.857155Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Average distance from home for currently active employees: {:.2f} miles and ex-employees: {:.2f} miles'.format(\n",
    "    df_HR[df_HR['Attrition'] == 'No']['DistanceFromHome'].mean(), df_HR[df_HR['Attrition'] == 'Yes']['DistanceFromHome'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:24:46.670496Z",
     "start_time": "2019-02-26T17:24:44.918647Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Add histogram data\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Extract data for histograms\n",
    "x1 = df_HR.loc[df_HR['Attrition'] == 'No', 'DistanceFromHome']\n",
    "x2 = df_HR.loc[df_HR['Attrition'] == 'Yes', 'DistanceFromHome']\n",
    "\n",
    "# Group data together\n",
    "hist_data = [x1, x2]\n",
    "group_labels = ['Active Employees', 'Ex-Employees']\n",
    "\n",
    "# Create histogram plot\n",
    "fig = go.Figure()\n",
    "\n",
    "for i, data in enumerate(hist_data):\n",
    "    fig.add_trace(go.Histogram(x=data, name=group_labels[i], nbinsx=15))\n",
    "\n",
    "# Add title\n",
    "fig.update_layout(title='Distance From Home Distribution in Percent by Attrition Status')\n",
    "fig.update_xaxes(range=[0, 30], dtick=2)\n",
    "\n",
    "# Plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Department"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The data features employee data from three departments: Research & Development, Sales, and Human Resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:25:51.988039Z",
     "start_time": "2019-02-26T17:25:51.976578Z"
    }
   },
   "outputs": [],
   "source": [
    "# The organisation consists of several departments\n",
    "df_HR['Department'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Department = pd.DataFrame(columns=[\"Department\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['Department'].unique()):\n",
    "    ratio = df_HR[(df_HR['Department']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['Department']==field].shape[0]\n",
    "    df_Department.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_DF = df_Department.groupby(by=\"Department\").sum()\n",
    "df_DF.iplot(kind='bar',title='Leavers by Department (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Role and Work Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A preliminary look at the relationship between Business Travel frequency and Attrition Status shows that there is a largest normalized proportion of Leavers for employees that travel \"frequently\". Travel metrics associated with Business Travel status were not disclosed (i.e. how many hours of Travel is considered \"Frequent\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:26:02.759590Z",
     "start_time": "2019-02-26T17:26:02.751269Z"
    }
   },
   "outputs": [],
   "source": [
    "# Employees have different business travel commitmnent depending on their roles and level in the organisation\n",
    "df_HR['BusinessTravel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BusinessTravel = pd.DataFrame(columns=[\"Business Travel\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['BusinessTravel'].unique()):\n",
    "    ratio = df_HR[(df_HR['BusinessTravel']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['BusinessTravel']==field].shape[0]\n",
    "    df_BusinessTravel.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_BT = df_BusinessTravel.groupby(by=\"Business Travel\").sum()\n",
    "df_BT.iplot(kind='bar',title='Leavers by Business Travel (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Several Job Roles are listed in the dataset: Sales Executive, Research Scientist, Laboratory Technician, Manufacturing Director, Healthcare Representative, Manager, Sales Representative, Research Director, Human Resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:32:43.178962Z",
     "start_time": "2019-02-26T17:32:43.171236Z"
    }
   },
   "outputs": [],
   "source": [
    "# Employees in the database have several roles on-file\n",
    "df_HR['JobRole'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_JobRole = pd.DataFrame(columns=[\"Job Role\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['JobRole'].unique()):\n",
    "    ratio = df_HR[(df_HR['JobRole']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['JobRole']==field].shape[0]\n",
    "    df_JobRole.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_JR = df_JobRole.groupby(by=\"Job Role\").sum()\n",
    "df_JR.iplot(kind='bar',title='Leavers by Job Role (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Employees have an assigned level within the organisation which varies from 1 (staff) to 5 (managerial/director). Employees with an assigned Job Level of \"1\" show the largest normalized proportion of Leavers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:32:53.123041Z",
     "start_time": "2019-02-26T17:32:53.112061Z"
    }
   },
   "outputs": [],
   "source": [
    "df_HR['JobLevel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_JobLevel = pd.DataFrame(columns=[\"Job Level\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['JobLevel'].unique()):\n",
    "    ratio = df_HR[(df_HR['JobLevel']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['JobLevel']==field].shape[0]\n",
    "    df_JobLevel.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_JL = df_JobLevel.groupby(by=\"Job Level\").sum()\n",
    "df_JL.iplot(kind='bar',title='Leavers by Job Level (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A ranking is associated to the employee's Job Involvement :1 'Low' 2 'Medium' 3 'High' 4 'Very High'. The plot below indicates a negative correlation with the Job Involvement of an employee and the Attrition Status. In other words, employees with higher Job Involvement are less likely to leave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:32:55.526061Z",
     "start_time": "2019-02-26T17:32:55.516931Z"
    }
   },
   "outputs": [],
   "source": [
    "df_HR['JobInvolvement'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_JobInvolvement = pd.DataFrame(columns=[\"Job Involvement\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['JobInvolvement'].unique()):\n",
    "    ratio = df_HR[(df_HR['JobInvolvement']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['JobInvolvement']==field].shape[0]\n",
    "    df_JobInvolvement.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_JI = df_JobInvolvement.groupby(by=\"Job Involvement\").sum()\n",
    "df_JI.iplot(kind='bar',title='Leavers by Job Involvement (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The data indicates that employees may have access to some Training. A feature indicates how many years it's been since the employee attended such training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:32:57.284487Z",
     "start_time": "2019-02-26T17:32:57.276298Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of training times last year varies from {} to {} years.\".format(\n",
    "    df_HR['TrainingTimesLastYear'].min(), df_HR['TrainingTimesLastYear'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add histogram data\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def create_histogram(dataframe, column_name, group_column, bin_size=1, title=None, x_range=None, x_dtick=None):\n",
    "    \"\"\"\n",
    "    Create a histogram plot with custom bin_size, title, x_range, and x_dtick.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pandas.DataFrame): The DataFrame containing the data.\n",
    "    column_name (str): The name of the column to create the histogram for.\n",
    "    group_column (str): The name of the column to group the data by.\n",
    "    bin_size (int, optional): The bin size for the histogram. Default is 1.\n",
    "    title (str, optional): The title of the plot. Default is None.\n",
    "    x_range (tuple, optional): The range of the x-axis. Default is None.\n",
    "    x_dtick (int, optional): The dtick value for the x-axis. Default is None.\n",
    "\n",
    "    Returns:\n",
    "    plotly.graph_objects.Figure: The histogram plot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract data for histograms\n",
    "    x1 = dataframe.loc[dataframe[group_column] == 'No', column_name]\n",
    "    x2 = dataframe.loc[dataframe[group_column] == 'Yes', column_name]\n",
    "\n",
    "    # Group data together\n",
    "    hist_data = [x1, x2]\n",
    "    group_labels = ['Active Employees', 'Ex-Employees']\n",
    "\n",
    "    # Create distplot with custom bin_size\n",
    "    fig = ff.create_distplot(hist_data, group_labels, bin_size=bin_size, curve_type='kde', show_hist=False, show_rug=False)\n",
    "\n",
    "    # Add title\n",
    "    if title:\n",
    "        fig.update_layout(title=title)\n",
    "\n",
    "    # Update x-axis\n",
    "    if x_range:\n",
    "        fig.update_xaxes(range=x_range)\n",
    "    if x_dtick:\n",
    "        fig.update_xaxes(dtick=x_dtick)\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Create the histogram for TrainingTimesLastYear\n",
    "fig = create_histogram(df_HR, 'TrainingTimesLastYear', 'Attrition', title='Training Times Last Year metric in Percent by Attrition Status', x_range=(0, 6), x_dtick=1)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There is a feature for the number of companies the employee has worked at. <br>\n",
    "> 0 likely indicates that according to records, the employee has only worked at this company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:32:58.858512Z",
     "start_time": "2019-02-26T17:32:58.848698Z"
    }
   },
   "outputs": [],
   "source": [
    "df_HR['NumCompaniesWorked'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NumCompaniesWorked = pd.DataFrame(columns=[\"Num Companies Worked\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['NumCompaniesWorked'].unique()):\n",
    "    ratio = df_HR[(df_HR['NumCompaniesWorked']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['NumCompaniesWorked']==field].shape[0]\n",
    "    df_NumCompaniesWorked.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_NC = df_NumCompaniesWorked.groupby(by=\"Num Companies Worked\").sum()\n",
    "df_NC.iplot(kind='bar',title='Leavers by Num Companies Worked (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Years at the Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average Number of Years at the company for currently active employees: {:.2f} miles and ex-employees: {:.2f} years'.format(\n",
    "    df_HR[df_HR['Attrition'] == 'No']['YearsAtCompany'].mean(), df_HR[df_HR['Attrition'] == 'Yes']['YearsAtCompany'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:04.945211Z",
     "start_time": "2019-02-26T17:33:04.939946Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of Years at the company varies from {} to {} years.\".format(\n",
    "    df_HR['YearsAtCompany'].min(), df_HR['YearsAtCompany'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:10.801401Z",
     "start_time": "2019-02-26T17:33:08.893465Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Add histogram data\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming df_HR is the DataFrame\n",
    "\n",
    "# Extract data for histograms\n",
    "x1 = df_HR.loc[df_HR['Attrition'] == 'No', 'YearsAtCompany']\n",
    "x2 = df_HR.loc[df_HR['Attrition'] == 'Yes', 'YearsAtCompany']\n",
    "\n",
    "# Group data together\n",
    "hist_data = [x1, x2]\n",
    "group_labels = ['Active Employees', 'Ex-Employees']\n",
    "\n",
    "# Create histogram figure\n",
    "fig = go.Figure()\n",
    "\n",
    "for i, data in enumerate(hist_data):\n",
    "    fig.add_trace(go.Histogram(x=data, name=group_labels[i], nbinsx=8))\n",
    "\n",
    "# Add title\n",
    "fig.update_layout(title='Years At Company in Percent by Attrition Status')\n",
    "fig.update_xaxes(range=[0, 40], dtick=5)\n",
    "\n",
    "# Plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:19.903172Z",
     "start_time": "2019-02-26T17:33:19.898332Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of Years in the current role varies from {} to {} years.\".format(\n",
    "    df_HR['YearsInCurrentRole'].min(), df_HR['YearsInCurrentRole'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add histogram data\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming df_HR is the DataFrame\n",
    "\n",
    "# Extract data for histograms\n",
    "x1 = df_HR.loc[df_HR['Attrition'] == 'No', 'YearsInCurrentRole']\n",
    "x2 = df_HR.loc[df_HR['Attrition'] == 'Yes', 'YearsInCurrentRole']\n",
    "\n",
    "# Group data together\n",
    "hist_data = [x1, x2]\n",
    "group_labels = ['Active Employees', 'Ex-Employees']\n",
    "\n",
    "# Create histogram figure\n",
    "fig = go.Figure()\n",
    "\n",
    "for i, data in enumerate(hist_data):\n",
    "    fig.add_trace(go.Histogram(x=data, name=group_labels[i], nbinsx=18))\n",
    "\n",
    "# Add title\n",
    "fig.update_layout(title='Years In Current Role in Percent by Attrition Status')\n",
    "fig.update_xaxes(range=[0, 18], dtick=1)\n",
    "\n",
    "# Plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:20.652097Z",
     "start_time": "2019-02-26T17:33:20.645465Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of Years since last promotion varies from {} to {} years.\".format(\n",
    "    df_HR['YearsSinceLastPromotion'].min(), df_HR['YearsSinceLastPromotion'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add histogram data\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming df_HR is the DataFrame\n",
    "\n",
    "# Extract data for histograms\n",
    "x1 = df_HR.loc[df_HR['Attrition'] == 'No', 'YearsSinceLastPromotion']\n",
    "x2 = df_HR.loc[df_HR['Attrition'] == 'Yes', 'YearsSinceLastPromotion']\n",
    "\n",
    "# Group data together\n",
    "hist_data = [x1, x2]\n",
    "group_labels = ['Active Employees', 'Ex-Employees']\n",
    "\n",
    "# Create histogram figure\n",
    "fig = go.Figure()\n",
    "\n",
    "for i, data in enumerate(hist_data):\n",
    "    fig.add_trace(go.Histogram(x=data, name=group_labels[i], nbinsx=15))\n",
    "\n",
    "# Add title\n",
    "fig.update_layout(title='Years Since Last Promotion in Percent by Attrition Status')\n",
    "fig.update_xaxes(range=[0, 15], dtick=1)\n",
    "\n",
    "# Plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:24.595137Z",
     "start_time": "2019-02-26T17:33:24.587030Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Total working years varies from {} to {} years.\".format(\n",
    "    df_HR['TotalWorkingYears'].min(), df_HR['TotalWorkingYears'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add histogram data\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Add histogram data\n",
    "x1 = df_HR.loc[df_HR['Attrition'] == 'No', 'TotalWorkingYears']\n",
    "x2 = df_HR.loc[df_HR['Attrition'] == 'Yes', 'TotalWorkingYears']\n",
    "\n",
    "# Group data together\n",
    "hist_data = [x1, x2]\n",
    "group_labels = ['Active Employees', 'Ex-Employees']\n",
    "\n",
    "# Create histogram plot\n",
    "fig = go.Figure()\n",
    "\n",
    "for i, data in enumerate(hist_data):\n",
    "    fig.add_trace(go.Histogram(x=data, name=group_labels[i], nbinsx=8))\n",
    "\n",
    "# Add title\n",
    "fig.update_layout(title='Total Working Years in Percent by Attrition Status')\n",
    "fig.update_xaxes(range=[0, 40], dtick=5)\n",
    "\n",
    "# Plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Years With Current Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average Number of Years wit current manager for currently active employees: {:.2f} miles and ex-employees: {:.2f} years'.format(\n",
    "    df_HR[df_HR['Attrition'] == 'No']['YearsWithCurrManager'].mean(), df_HR[df_HR['Attrition'] == 'Yes']['YearsWithCurrManager'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:28.589679Z",
     "start_time": "2019-02-26T17:33:28.582117Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of Years wit current manager varies from {} to {} years.\".format(\n",
    "    df_HR['YearsWithCurrManager'].min(), df_HR['YearsWithCurrManager'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:31.588320Z",
     "start_time": "2019-02-26T17:33:28.592985Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Add histogram data\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming df_HR is the DataFrame\n",
    "\n",
    "# Extract data for histograms\n",
    "x1 = df_HR.loc[df_HR['Attrition'] == 'No', 'YearsWithCurrManager']\n",
    "x2 = df_HR.loc[df_HR['Attrition'] == 'Yes', 'YearsWithCurrManager']\n",
    "\n",
    "# Group data together\n",
    "hist_data = [x1, x2]\n",
    "group_labels = ['Active Employees', 'Ex-Employees']\n",
    "\n",
    "# Create histogram figure\n",
    "fig = go.Figure()\n",
    "\n",
    "for i, data in enumerate(hist_data):\n",
    "    fig.add_trace(go.Histogram(x=data, name=group_labels[i], nbinsx=17))\n",
    "\n",
    "# Add title\n",
    "fig.update_layout(title='Years With Curr Manager in Percent by Attrition Status')\n",
    "fig.update_xaxes(range=[0, 17], dtick=1)\n",
    "\n",
    "# Plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work-Life Balance Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A feature related to \"Work-Life Balance\" was captured as: 1 'Bad' 2 'Good' 3 'Better' 4 'Best'. The data indicates that the largest normalised proportion of Leavers had \"Bad\" Work-Life Balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:37.907622Z",
     "start_time": "2019-02-26T17:33:37.896498Z"
    }
   },
   "outputs": [],
   "source": [
    "df_HR['WorkLifeBalance'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:40.167381Z",
     "start_time": "2019-02-26T17:33:38.117086Z"
    }
   },
   "outputs": [],
   "source": [
    "df_WorkLifeBalance = pd.DataFrame(columns=[\"WorkLifeBalance\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['WorkLifeBalance'].unique()):\n",
    "    ratio = df_HR[(df_HR['WorkLifeBalance']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['WorkLifeBalance']==field].shape[0]\n",
    "    df_WorkLifeBalance.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_WLB = df_WorkLifeBalance.groupby(by=\"WorkLifeBalance\").sum()\n",
    "df_WLB.iplot(kind='bar',title='Leavers by WorkLifeBalance (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> All employees have a standard 80-hour work commitment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:42.180449Z",
     "start_time": "2019-02-26T17:33:42.167857Z"
    }
   },
   "outputs": [],
   "source": [
    "df_HR['StandardHours'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Some employees have overtime commitments. The data clearly show that there is significant larger portion of employees with OT that have left the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:24.613190Z",
     "start_time": "2019-02-26T17:33:24.598613Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_HR['OverTime'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_OverTime = pd.DataFrame(columns=[\"OverTime\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['OverTime'].unique()):\n",
    "    ratio = df_HR[(df_HR['OverTime']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['OverTime']==field].shape[0]\n",
    "    df_OverTime.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_OT = df_OverTime.groupby(by=\"OverTime\").sum()\n",
    "df_OT.iplot(kind='bar',title='Leavers by OverTime (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pay/Salary Employee Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:45.871154Z",
     "start_time": "2019-02-26T17:33:45.863994Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Employee Hourly Rate varies from ${} to ${}.\".format(\n",
    "    df_HR['HourlyRate'].min(), df_HR['HourlyRate'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:47.521697Z",
     "start_time": "2019-02-26T17:33:47.513959Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Employee Daily Rate varies from ${} to ${}.\".format(\n",
    "    df_HR['DailyRate'].min(), df_HR['DailyRate'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:49.163333Z",
     "start_time": "2019-02-26T17:33:49.155766Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Employee Monthly Rate varies from ${} to ${}.\".format(\n",
    "    df_HR['MonthlyRate'].min(), df_HR['MonthlyRate'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:50.559036Z",
     "start_time": "2019-02-26T17:33:50.551761Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Employee Monthly Income varies from ${} to ${}.\".format(\n",
    "    df_HR['MonthlyIncome'].min(), df_HR['MonthlyIncome'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:52.785447Z",
     "start_time": "2019-02-26T17:33:50.680438Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Add histogram data\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming df_HR is the DataFrame\n",
    "\n",
    "# Extract data for histograms\n",
    "x1 = df_HR.loc[df_HR['Attrition'] == 'No', 'MonthlyIncome']\n",
    "x2 = df_HR.loc[df_HR['Attrition'] == 'Yes', 'MonthlyIncome']\n",
    "\n",
    "# Group data together\n",
    "hist_data = [x1, x2]\n",
    "group_labels = ['Active Employees', 'Ex-Employees']\n",
    "\n",
    "# Create histogram figure\n",
    "fig = go.Figure()\n",
    "\n",
    "for i, data in enumerate(hist_data):\n",
    "    fig.add_trace(go.Histogram(x=data, name=group_labels[i], nbinsx=100))\n",
    "\n",
    "# Add title\n",
    "fig.update_layout(title='Monthly Income by Attrition Status')\n",
    "fig.update_xaxes(range=[0, 20000], dtick=2000)\n",
    "\n",
    "# Plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:33:57.173763Z",
     "start_time": "2019-02-26T17:33:57.160269Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Percentage Salary Hikes varies from {}% to {}%.\".format(\n",
    "    df_HR['PercentSalaryHike'].min(), df_HR['PercentSalaryHike'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:01.284384Z",
     "start_time": "2019-02-26T17:33:59.196505Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Add histogram data\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming df_HR is the DataFrame\n",
    "\n",
    "# Extract data for histograms\n",
    "x1 = df_HR.loc[df_HR['Attrition'] == 'No', 'PercentSalaryHike']\n",
    "x2 = df_HR.loc[df_HR['Attrition'] == 'Yes', 'PercentSalaryHike']\n",
    "\n",
    "# Group data together\n",
    "hist_data = [x1, x2]\n",
    "group_labels = ['Active Employees', 'Ex-Employees']\n",
    "\n",
    "# Create histogram figure\n",
    "fig = go.Figure()\n",
    "\n",
    "for i, data in enumerate(hist_data):\n",
    "    fig.add_trace(go.Histogram(x=data, name=group_labels[i], nbinsx=16))\n",
    "\n",
    "# Add title\n",
    "fig.update_layout(title='Percent Salary Hike by Attrition Status')\n",
    "fig.update_xaxes(range=[10, 26], dtick=1)\n",
    "\n",
    "# Plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:01.300618Z",
     "start_time": "2019-02-26T17:34:01.288154Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Stock Option Levels varies from {} to {}.\".format(\n",
    "    df_HR['StockOptionLevel'].min(), df_HR['StockOptionLevel'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:05.306907Z",
     "start_time": "2019-02-26T17:34:05.281163Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Normalised percentage of leavers by Stock Option Level: 1: {:.2f}%, 2: {:.2f}%, 3: {:.2f}%\".format(\n",
    "    df_HR[(df_HR['Attrition'] == 'Yes') & (df_HR['StockOptionLevel'] == 1)\n",
    "          ].shape[0] / df_HR[df_HR['StockOptionLevel'] == 1].shape[0]*100,\n",
    "    df_HR[(df_HR['Attrition'] == 'Yes') & (df_HR['StockOptionLevel'] == 2)\n",
    "          ].shape[0] / df_HR[df_HR['StockOptionLevel'] == 1].shape[0]*100,\n",
    "    df_HR[(df_HR['Attrition'] == 'Yes') & (df_HR['StockOptionLevel'] == 3)].shape[0] / df_HR[df_HR['StockOptionLevel'] == 1].shape[0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:07.325700Z",
     "start_time": "2019-02-26T17:34:05.492995Z"
    }
   },
   "outputs": [],
   "source": [
    "df_StockOptionLevel = pd.DataFrame(columns=[\"StockOptionLevel\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['StockOptionLevel'].unique()):\n",
    "    ratio = df_HR[(df_HR['StockOptionLevel']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['StockOptionLevel']==field].shape[0]\n",
    "    df_StockOptionLevel.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_SOL = df_StockOptionLevel.groupby(by=\"StockOptionLevel\").sum()\n",
    "df_SOL.iplot(kind='bar',title='Leavers by Stock Option Level (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Employee Satisfaction and Performance Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Environment Satisfaction was captured as: 1 'Low' 2 'Medium' 3 'High' 4 'Very High'. <br> \n",
    "Proportion of Leaving Employees decreases as the Environment Satisfaction score increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:10.643374Z",
     "start_time": "2019-02-26T17:34:10.633995Z"
    }
   },
   "outputs": [],
   "source": [
    "df_HR['EnvironmentSatisfaction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_EnvironmentSatisfaction = pd.DataFrame(columns=[\"EnvironmentSatisfaction\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['EnvironmentSatisfaction'].unique()):\n",
    "    ratio = df_HR[(df_HR['EnvironmentSatisfaction']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['EnvironmentSatisfaction']==field].shape[0]\n",
    "    df_EnvironmentSatisfaction.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_Env = df_EnvironmentSatisfaction.groupby(by=\"EnvironmentSatisfaction\").sum()\n",
    "df_Env.iplot(kind='bar',title='Leavers by Environment Satisfaction (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Job Satisfaction was captured as: 1 'Low' 2 'Medium' 3 'High' 4 'Very High'. <br> \n",
    "Proportion of Leaving Employees decreases as the Job Satisfaction score increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:13.476654Z",
     "start_time": "2019-02-26T17:34:13.462306Z"
    }
   },
   "outputs": [],
   "source": [
    "# Job Satisfaction was captured as: 1 'Low' 2 'Medium' 3 'High' 4 'Very High'\n",
    "df_HR['JobSatisfaction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:20.063756Z",
     "start_time": "2019-02-26T17:34:18.049645Z"
    }
   },
   "outputs": [],
   "source": [
    "df_JobSatisfaction = pd.DataFrame(columns=[\"JobSatisfaction\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['JobSatisfaction'].unique()):\n",
    "    ratio = df_HR[(df_HR['JobSatisfaction']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['JobSatisfaction']==field].shape[0]\n",
    "    df_JobSatisfaction.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_JS = df_JobSatisfaction.groupby(by=\"JobSatisfaction\").sum()\n",
    "df_JS.iplot(kind='bar',title='Leavers by Job Satisfaction (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Relationship Satisfaction was captured as: 1 'Low', 2 'Medium', 3 'High', 4 'Very High'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:21.624859Z",
     "start_time": "2019-02-26T17:34:21.615539Z"
    }
   },
   "outputs": [],
   "source": [
    "df_HR['RelationshipSatisfaction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RelationshipSatisfaction = pd.DataFrame(columns=[\"RelationshipSatisfaction\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['RelationshipSatisfaction'].unique()):\n",
    "    ratio = df_HR[(df_HR['RelationshipSatisfaction']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['RelationshipSatisfaction']==field].shape[0]\n",
    "    df_RelationshipSatisfaction.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_RS = df_RelationshipSatisfaction.groupby(by=\"RelationshipSatisfaction\").sum()\n",
    "df_RS.iplot(kind='bar',title='Leavers by Relationship Satisfaction (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Employee Performance Rating was captured as: 1 'Low' 2 'Good' 3 'Excellent' 4 'Outstanding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:23.293321Z",
     "start_time": "2019-02-26T17:34:23.284867Z"
    }
   },
   "outputs": [],
   "source": [
    "df_HR['PerformanceRating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:23.314235Z",
     "start_time": "2019-02-26T17:34:23.296543Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Normalised percentage of leavers by Stock Option Level: 3: {:.2f}%, 4: {:.2f}%\".format(\n",
    "    df_HR[(df_HR['Attrition'] == 'Yes') & (df_HR['PerformanceRating'] == 3)\n",
    "          ].shape[0] / df_HR[df_HR['StockOptionLevel'] == 1].shape[0]*100,\n",
    "    df_HR[(df_HR['Attrition'] == 'Yes') & (df_HR['PerformanceRating'] == 4)].shape[0] / df_HR[df_HR['StockOptionLevel'] == 1].shape[0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:26.576253Z",
     "start_time": "2019-02-26T17:34:24.766264Z"
    }
   },
   "outputs": [],
   "source": [
    "df_PerformanceRating = pd.DataFrame(columns=[\"PerformanceRating\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_HR['PerformanceRating'].unique()):\n",
    "    ratio = df_HR[(df_HR['PerformanceRating']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] / df_HR[df_HR['PerformanceRating']==field].shape[0]\n",
    "    df_PerformanceRating.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \n",
    "df_PR = df_PerformanceRating.groupby(by=\"PerformanceRating\").sum()\n",
    "df_PR.iplot(kind='bar',title='Leavers by Performance Rating (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variable: Attrition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The feature 'Attrition' is what this Machine Learning problem is about. We are trying to predict the value of the feature 'Attrition' by using other related features associated with the employee's personal and professional history. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:30.057344Z",
     "start_time": "2019-02-26T17:34:30.044363Z"
    }
   },
   "outputs": [],
   "source": [
    "# Attrition indicates if the employee is currently active ('No') or has left the company ('Yes')\n",
    "df_HR['Attrition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:31.567472Z",
     "start_time": "2019-02-26T17:34:31.556605Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Percentage of Current Employees is {:.1f}% and of Ex-employees is: {:.1f}%\".format(\n",
    "    df_HR[df_HR['Attrition'] == 'No'].shape[0] / df_HR.shape[0]*100,\n",
    "    df_HR[df_HR['Attrition'] == 'Yes'].shape[0] / df_HR.shape[0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:34:33.217588Z",
     "start_time": "2019-02-26T17:34:33.076203Z"
    }
   },
   "outputs": [],
   "source": [
    "df_HR['Attrition'].iplot(kind='hist', xTitle='Attrition',\n",
    "                         yTitle='count', title='Attrition Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As shown on the chart above, we see this is an imbalanced class problem. Indeed, the percentage of Current Employees in our dataset is 83.9% and the percentage of Ex-employees is: 16.1%\n",
    "\n",
    "> Machine learning algorithms typically work best when the number of instances of each classes are roughly equal. We will have to address this target feature imbalance prior to implementing our Machine Learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's take a look at some of most significant correlations. It is worth remembering that correlation coefficients only measure linear correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:43:13.261360Z",
     "start_time": "2019-02-26T17:43:13.224637Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_columns = df_HR.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Calculate correlations\n",
    "corr = df_HR[numeric_columns].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr, cmap=\"YlGnBu\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's plot a heatmap to visualize the correlation between Attrition and these factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T17:44:13.037172Z",
     "start_time": "2019-02-26T17:44:12.365082Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df_HR is defined or imported correctly\n",
    "# If you need to import data, replace this line with your data import code\n",
    "# Example: df_HR = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Handling non-numeric values\n",
    "numeric_columns = df_HR.select_dtypes(include=[np.number])\n",
    "# If there are non-numeric columns that need to be handled, you can do so here\n",
    "# For example, converting categorical columns to numeric using one-hot encoding\n",
    "\n",
    "# Calculate correlations\n",
    "corr = numeric_columns.corr()\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(corr, mask=mask, vmax=.5, annot=True, fmt='.2f', linewidths=.2, cmap=\"YlGnBu\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As shown above, \"Monthly Rate\", \"Number of Companies Worked\" and \"Distance From Home\" are positively correlated to Attrition; <br> while \"Total Working Years\", \"Job Level\", and \"Years In Current Role\" are negatively correlated to Attrition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA Concluding Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summarise the findings from this EDA: <br>\n",
    "\n",
    "> - The dataset does not feature any missing or erroneous data values, and all features are of the correct data type. <br>\n",
    "- The strongest positive correlations with the target features are: **Performance Rating**, **Monthly Rate**, **Num Companies Worked**, **Distance From Home**. \n",
    "- The strongest negative correlations with the target features are: **Total Working Years**, **Job Level**, **Years In Current Role**, and **Monthly Income**.\n",
    "- The dataset is **imbalanced** with the majoriy of observations describing Currently Active Employees. <br>\n",
    "- Several features (ie columns) are redundant for our analysis, namely: EmployeeCount, EmployeeNumber, StandardHours, and Over18. <br>\n",
    "\n",
    "Other observations include: <br>\n",
    "> - Single employees show the largest proportion of leavers, compared to Married and Divorced counterparts. <br>\n",
    "- About 10% of leavers left when they reach their 2-year anniversary at the company. <br>\n",
    "- Loyal employees with higher salaries and more responsbilities show lower proportion of leavers compared to their counterparts. <br>\n",
    "- People who live further away from their work show higher proportion of leavers compared to their counterparts.<br>\n",
    "- People who travel frequently show higher proportion of leavers compared to their counterparts.<br>\n",
    "- People who have to work overtime show higher proportion of leavers compared to their counterparts.<br>\n",
    "- Employee who work as Sales Representatives show a significant percentage of Leavers in the submitted dataset.<br>\n",
    "- Employees that have already worked at several companies previously (already \"bounced\" between workplaces) show higher proportion of leavers compared to their counterparts.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://images.unsplash.com/photo-1498409785966-ab341407de6e?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1360&q=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we undertake data pre-processing steps to prepare the datasets for Machine Learning algorithm implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Machine Learning algorithms can typically only have numerical values as their predictor variables. Hence Label Encoding becomes necessary as they encode categorical labels with numerical values. To avoid introducing feature importance for categorical features with large numbers of unique values, we will use both Lable Encoding and One-Hot Encoding as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# Create a label encoder object\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_HR.shape)\n",
    "df_HR.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding will be used for columns with 2 or less unique values\n",
    "le_count = 0\n",
    "for col in df_HR.columns[1:]:\n",
    "    if df_HR[col].dtype == 'object':\n",
    "        if len(list(df_HR[col].unique())) <= 2:\n",
    "            le.fit(df_HR[col])\n",
    "            df_HR[col] = le.transform(df_HR[col])\n",
    "            le_count += 1\n",
    "print('{} columns were label encoded.'.format(le_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert rest of categorical variable into dummy\n",
    "df_HR = pd.get_dummies(df_HR, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The resulting dataframe has **49 columns** for 1,470 employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_HR.shape)\n",
    "df_HR.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Feature Scaling using MinMaxScaler essentially shrinks the range such that the range is now between 0 and n. Machine Learning algorithms perform better when input numerical variables fall within a similar scale. In this case, we are scaling between 0 and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 5))\n",
    "HR_col = list(df_HR.columns)\n",
    "HR_col.remove('Attrition')\n",
    "for col in HR_col:\n",
    "    df_HR[col] = df_HR[col].astype(float)\n",
    "    df_HR[[col]] = scaler.fit_transform(df_HR[[col]])\n",
    "df_HR['Attrition'] = pd.to_numeric(df_HR['Attrition'], downcast='float')\n",
    "df_HR.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Size of Full Encoded Dataset: {}'. format(df_HR.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Prior to implementating or applying any Machine Learning algorithms, we must decouple training and testing datasets from our master dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the target to a new dataframe and convert it to a numerical feature\n",
    "#df_target = df_HR[['Attrition']].copy()\n",
    "target = df_HR['Attrition'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's remove the target feature and redundant features from the dataset\n",
    "df_HR.drop(['Attrition', 'EmployeeCount', 'EmployeeNumber',\n",
    "            'StandardHours', 'Over18'], axis=1, inplace=True)\n",
    "print('Size of Full dataset is: {}'.format(df_HR.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming df_HR and target are defined correctly\n",
    "\n",
    "# Since we have class imbalance (i.e. more employees with turnover=0 than turnover=1)\n",
    "# let's use stratify=y to maintain the same ratio as in the training dataset when splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_HR,\n",
    "                                                    target,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=7,\n",
    "                                                    stratify=target)  \n",
    "\n",
    "print(\"Number transactions X_train dataset: \", X_train.shape)\n",
    "print(\"Number transactions y_train dataset: \", y_train.shape)\n",
    "print(\"Number transactions X_test dataset: \", X_test.shape)\n",
    "print(\"Number transactions y_test dataset: \", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's first use a range of **baseline** algorithms (using out-of-the-box hyper-parameters) before we move on to more sophisticated solutions. The algorithms considered in this section are: **Logistic Regression**, **Random Forest**, **SVM**, **KNN**, **Decision Tree Classifier**, **Gaussian NB**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# selection of algorithms to consider and set performance measure\n",
    "models = []\n",
    "models.append(('Logistic Regression', LogisticRegression(solver='liblinear', random_state=7,\n",
    "                                                         class_weight='balanced')))\n",
    "models.append(('Random Forest', RandomForestClassifier(n_estimators=100, random_state=7)))\n",
    "models.append(('SVM', SVC(gamma='auto', random_state=7)))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('Decision Tree Classifier', DecisionTreeClassifier(random_state=7)))\n",
    "models.append(('Gaussian NB', GaussianNB()))\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's evaluate each model in turn and provide accuracy and standard deviation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Assuming the classifiers (models) are defined correctly above\n",
    "# Assuming X_train and y_train are defined correctly\n",
    "\n",
    "acc_results = []\n",
    "auc_results = []\n",
    "names = []\n",
    "\n",
    "# set table to table to populate with performance results\n",
    "col = ['Algorithm', 'ROC AUC Mean', 'ROC AUC STD', 'Accuracy Mean', 'Accuracy STD']\n",
    "df_results = pd.DataFrame(columns=col)\n",
    "i = 0\n",
    "\n",
    "# evaluate each model using cross-validation\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(\n",
    "        n_splits=10, shuffle=True, random_state=7)  # 10-fold cross-validation with shuffle=True\n",
    "\n",
    "    cv_acc_results = model_selection.cross_val_score(  # accuracy scoring\n",
    "        model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "\n",
    "    cv_auc_results = model_selection.cross_val_score(  # roc_auc scoring\n",
    "        model, X_train, y_train, cv=kfold, scoring='roc_auc')\n",
    "\n",
    "    acc_results.append(cv_acc_results)\n",
    "    auc_results.append(cv_auc_results)\n",
    "    names.append(name)\n",
    "    df_results.loc[i] = [name,\n",
    "                         round(cv_auc_results.mean()*100, 2),\n",
    "                         round(cv_auc_results.std()*100, 2),\n",
    "                         round(cv_acc_results.mean()*100, 2),\n",
    "                         round(cv_acc_results.std()*100, 2)\n",
    "                         ]\n",
    "    i += 1\n",
    "\n",
    "# Sort the DataFrame by 'ROC AUC Mean' column\n",
    "df_results = df_results.sort_values(by=['ROC AUC Mean'], ascending=False)\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Classification Accuracy** is the number of correct predictions made as a ratio of all predictions made. <br> \n",
    "It is the most common evaluation metric for classification problems. However, it is often **misused** as it is only really suitable when there are an **equal number of observations in each class** and all predictions and prediction errors are equally important. It is not the case in this project, so a different scoring metric may be more suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 7))\n",
    "fig.suptitle('Algorithm Accuracy Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(acc_results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Area under ROC Curve** (or AUC for short) is a performance metric for binary classification problems. <br>\n",
    "The AUC represents a **modelâ€™s ability to discriminate between positive and negative classes**. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 7))\n",
    "fig.suptitle('Algorithm ROC AUC Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(auc_results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Based on our ROC AUC comparison analysis, **Logistic Regression** and **Random Forest** show the highest mean AUC scores. We will shortlist these two algorithms for further analysis. See below for more details on these two algos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression** is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. Logistic Regression is classification algorithm that is not as sophisticated as the ensemble methods or boosted decision trees method discussed below. Hence, it provides us with a good benchmark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://cdn-images-1.medium.com/max/1600/0*vRhSdZ_k4wrP6Bl8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest** is a popular and versatile machine learning method that is capable of solving both regression and classification. Random Forest is a brand of Ensemble learning, as it relies on an ensemble of decision trees. It aggregates Classification (or Regression) Trees. A decision tree is composed of a series of decisions that can be used to classify an observation in a dataset.\n",
    "\n",
    "Random Forest fits a number of decision tree classifiers on various **sub-samples of the dataset** and use **averaging** to improve the predictive accuracy and control over-fitting. Random Forest can handle a large number of features, and is helpful for estimating which of your variables are important in the underlying data being modeled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://images.unsplash.com/photo-1441422454217-519d3ee81350?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1489&q=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's take a closer look at using the Logistic Regression algorithm. I'll be using 10 fold Cross-Validation to train our Logistic Regression Model and estimate its AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Assuming X_train and y_train are defined correctly\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, shuffle=True, random_state=7)  # Corrected\n",
    "\n",
    "modelCV = LogisticRegression(solver='liblinear', class_weight=\"balanced\", random_state=7)\n",
    "scoring = 'roc_auc'\n",
    "results = model_selection.cross_val_score(\n",
    "    modelCV, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "print(\"AUC score (STD): %.2f (%.2f)\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> GridSearchCV allows use to fine-tune hyper-parameters by searching over specified parameter values for an estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Assuming X_train and y_train are defined correctly\n",
    "\n",
    "param_grid = {'C': np.arange(1e-03, 2, 0.01)}  # hyper-parameter list to fine-tune\n",
    "log_gs = GridSearchCV(LogisticRegression(solver='liblinear',  # setting GridSearchCV\n",
    "                                          class_weight=\"balanced\", \n",
    "                                          random_state=7),\n",
    "                      return_train_score=True,\n",
    "                      param_grid=param_grid,\n",
    "                      scoring='roc_auc',\n",
    "                      cv=10)\n",
    "\n",
    "log_grid = log_gs.fit(X_train, y_train)\n",
    "log_opt = log_grid.best_estimator_\n",
    "results = log_gs.cv_results_\n",
    "\n",
    "print('='*20)\n",
    "print(\"best params: \" + str(log_gs.best_estimator_))\n",
    "print(\"best params: \" + str(log_gs.best_params_))\n",
    "print('best score:', log_gs.best_score_)\n",
    "print('='*20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As shown above, the results from GridSearchCV provided us with fine-tuned hyper-parameter using ROC_AUC as the scoring metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Confusion Matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "\n",
    "# Assuming y_test and log_opt are defined correctly\n",
    "\n",
    "# Confusion Matrix\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, log_opt.predict(X_test))\n",
    "class_names = [0, 1]  # name of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of Logistic Regression Classifier on test set: {:.2f}'.format(log_opt.score(X_test, y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The Confusion matrix is telling us that we have 231+47 correct predictions and 78+12 incorrect predictions. In other words, an accurac of 75.54%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Classification report for the optimised Log Regression\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming log_opt, X_train, and y_train are defined correctly\n",
    "\n",
    "# Fit the optimized logistic regression model on the training data\n",
    "log_opt.fit(X_train, y_train)\n",
    "\n",
    "# Print the classification report for the predictions made by the model on the test data\n",
    "print(classification_report(y_test, log_opt.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Instead of getting binary estimated target features (0 or 1), a probability can be associated with the predicted target. <br> The output provides a first index referring to the probability that the data belong to **class 0** (employee not leaving), and the second refers to the probability that the data belong to **class 1** (employee leaving).\n",
    "\n",
    "> The resulting AUC score is: 0.857 which is higher than that best score during the optimisation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Fit the optimized model to the training data\n",
    "log_opt.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "probs = log_opt.predict_proba(X_test)\n",
    "\n",
    "# Extract probabilities associated with the positive class (employee leaving)\n",
    "probs = probs[:, 1]\n",
    "\n",
    "# Calculate AUC score using test dataset\n",
    "logit_roc_auc = roc_auc_score(y_test, probs)\n",
    "\n",
    "# Print AUC score\n",
    "print('AUC score: %.3f' % logit_roc_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's take a closer look at using the Random Forest algorithm. I'll fine-tune the Random Forest algorithm's hyper-parameters by cross-validation against the AUC score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the company generates more data on its employees (on New Joiners and recent Leavers) the algorithm can be re-trained using the additional data and theoritically generate more accurate predictions to identify **high-risk employees** of leaving based on the probabilistic label assigned to each feature variable (i.e. employee) by the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Employees can be assigning a \"Risk Category\" based on the predicted label such that:\n",
    "- **Low-risk** for employees with label < 0.6\n",
    "- **Medium-risk** for employees with label between 0.6 and 0.8\n",
    "- **High-risk** for employees with label > 0.8 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://images.unsplash.com/photo-1535017584024-2f4bead257df?ixlib=rb-1.2.1&auto=format&fit=crop&w=1350&q=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategic Retention Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The stronger indicators of people leaving include:\n",
    "    - **Monthly Income**: people on higher wages are less likely to leave the company. Hence, efforts should be made to gather information on industry benchmarks in the current local market to determine if the company is providing competitive wages.\n",
    "    - **Over Time**: people who work overtime are more likelty to leave the company. Hence efforts  must be taken to appropriately scope projects upfront with adequate support and manpower so as to reduce the use of overtime.\n",
    "    - **YearsWithCurrManager**: A large number of leavers leave 6 months after their Current Managers. By using Line Manager details for each employee, one can determine which Manager have experienced the largest numbers of employees resigning over the past year. Several metrics can be used here to determine whether action should be taken with a Line Manager: \n",
    "        - number of employees under managers showing high turnover rates: this would indicate that the organisation's structure may need to be revisit to improve efficiency\n",
    "        - number of years the Line Manager has been in a particular position: this may indicate that the employees may need management training or be assigned a mentor (ideally an Executive) in the organisation\n",
    "        - patterns in the employees who have resigned: this may indicate recurring patterns in employees leaving in which case action may be taken accordingly.\n",
    "    - **Age**: Employees in relatively young age bracket 25-35 are more likely to leave. Hence, efforts should be made to clearly articulate the long-term vision of the company and young employees fit in that vision, as well as provide incentives in the form of clear paths to promotion for instance.\n",
    "    - **DistanceFromHome**: Employees who live further from home are more likely to leave the company. Hence, efforts should be made to provide support in the form of company transportation for clusters of employees leaving the same area, or in the form of Transportation Allowance. Initial screening of employees based on their home location is probably not recommended as it would be regarded as a form of discrimination as long as employees make it to work on time every day.\n",
    "    - **TotalWorkingYears**: The more experienced employees are less likely to leave. Employees who have between 5-8 years of experience should be identified as potentially having a higher-risk of leaving.\n",
    "    - **YearsAtCompany**: Loyal companies are less likely to leave. Employees who hit their two-year anniversary should be identified as potentially having a higher-risk of leaving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A strategic **\"Retention Plan\"** should be drawn for each **Risk Category** group. In addition to the suggested steps for each feature listed above, face-to-face meetings between a HR representative and employees can be initiated for **medium-** and **high-risk employees** to discuss work conditions. Also, a meeting with those employee's Line Manager would allow to discuss the work environment within the team and whether steps can be taken to improve it."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "296px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
